{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекомендация статей с помощью тематических моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from collections import OrderedDict\n",
    "from gensim.models.ldamodel import LdaModel, CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import string\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_tokens = OrderedDict()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "with open('data/arxiv_plain.txt', 'r') as f:\n",
    "    for line in tqdm.tqdm(f):\n",
    "        cur_tokens = line.split()\n",
    "        arxiv_tokens[cur_tokens[0]] = list(filter(lambda token: token not in stop_words, cur_tokens[1:]))\n",
    "arxiv_titles = list(arxiv_tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_dictionary = Dictionary(list(arxiv_tokens.values()))\n",
    "arxiv_corpus = [arxiv_dictionary.doc2bow(text) for text in list(arxiv_tokens.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(arxiv_corpus, num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.get_document_topics(arxiv_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = {}\n",
    "for doc_title, doc_bow in tqdm.tqdm(zip(arxiv_titles, arxiv_corpus)):\n",
    "    topic_vector = np.zeros(lda.num_topics)\n",
    "    for topic_num, topic_prob in lda.get_document_topics(doc_bow):\n",
    "        topic_vector[topic_num] = topic_prob\n",
    "    theta[doc_title] = topic_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тематический вектор статьи с номером 0704.0004:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta['0704.0004']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь для того, чтобы порекомендовать читателю близкие по смыслу статьи, достаточно выбрать метрику близости и сравнить вектор текущего документа (например, последнего прочитанного) с векторами всех остальных документов в коллекции. В качестве метрики близости можно использовать косинусную меру, евклидово расстояние, расстояние Хелингера и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(first, second):\n",
    "    return metrics.pairwise.cosine_similarity(first.reshape(1, -1), second.reshape(1, -1))[0][0]\n",
    "\n",
    "def dot_sim(first, second):\n",
    "    return first.dot(second)\n",
    "\n",
    "def hel_sim(first, second): #one more sqrt and division by sqrt(2) omitted, minus added\n",
    "    return -np.sum((np.sqrt(first) - np.sqrt(second)) ** 2)\n",
    "\n",
    "def jaccard_sim(first, second):\n",
    "    intersection = set(first).intersection(set(second))\n",
    "    union = set(first).union(set(second))\n",
    "    return float(len(intersection))/float(len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_papers(query, theta, sim=cos_sim, top_k=10):\n",
    "    query_vec = theta[query]\n",
    "    ranked_list = []\n",
    "    for doc_name, doc_vec in theta.items():\n",
    "        ranked_list.append((doc_name, sim(query_vec, doc_vec)))\n",
    "    ranked_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    return ranked_list[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_papers = recommend_papers('0704.2596', theta, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_name, prob in recommended_papers:\n",
    "    print(paper_name)\n",
    "    print(' '.join([token[0] for token in Counter(arxiv_tokens[paper_name]).most_common(10)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки качества полученной рекомендательной системы воспользуемся датасетом триплетов [[Dai et al. 2015](https://arxiv.org/abs/1507.07998)]. Датасет содержит тройки статей `<запрос>|<релевантная статья>|<нерелевантная статья>`. Будем считать, что если метрика близости между запросом и релевантной статьей оказалась выше, чем между запросом и нерелевантной статьей, то такая тройка обработана \"правильно\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(theta, sim):\n",
    "    all_triplets = 0\n",
    "    covered_triplets = 0\n",
    "    correct_triplets = 0\n",
    "    with open('data/arxiv_triplets.txt', 'r') as fin:\n",
    "        for line in fin:\n",
    "            ids = list(map(lambda x: x.split('/pdf/')[-1], line.split()))\n",
    "            if all([x in theta.keys() for x in ids]):\n",
    "                covered_triplets += 1\n",
    "                vectors = [theta[x] for x in ids]\n",
    "                correct_triplets += sim(vectors[0], vectors[1]) > sim(vectors[0], vectors[2])\n",
    "            all_triplets += 1\n",
    "    return 1.0 * correct_triplets / covered_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_quality(theta, cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_quality(theta, hel_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_quality(theta, dot_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем 300 тем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(arxiv_corpus, num_topics=300)\n",
    "\n",
    "theta = {}\n",
    "for doc_title, doc_bow in tqdm.tqdm(zip(arxiv_titles, arxiv_corpus)):\n",
    "    topic_vector = np.zeros(lda.num_topics)\n",
    "    for topic_num, topic_prob in lda.get_document_topics(doc_bow):\n",
    "        topic_vector[topic_num] = topic_prob\n",
    "    theta[doc_title] = topic_vector\n",
    "    \n",
    "print(evaluate_quality(theta, cos_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперимент №2: использовать BERT-based фичи совместно с тематическими фичами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from scipy import sparse\n",
    "from nltk import word_tokenize\n",
    "import json\n",
    "import torch\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT: http://jalammar.github.io/illustrated-bert/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading abtracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/arxiv_metadata.json', 'r') as f:\n",
    "    data = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43091it [00:14, 2991.13it/s]\n"
     ]
    }
   ],
   "source": [
    "arxiv_titles = []\n",
    "with open('data/arxiv_plain.txt', 'r') as f:\n",
    "    for line in tqdm.tqdm(f):\n",
    "        cur_tokens = line.split()\n",
    "        arxiv_titles.append(cur_tokens[0])\n",
    "arxiv_titles = set(arxiv_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1796911/1796911 [01:01<00:00, 29196.28it/s]\n"
     ]
    }
   ],
   "source": [
    "arxiv_tokens = {}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for article in tqdm.tqdm(data[:-1]):\n",
    "    arxiv_json =  json.loads(article)\n",
    "    if arxiv_json['id'] not in arxiv_titles:\n",
    "        continue\n",
    "    \n",
    "    text = arxiv_json['abstract'].lower()\n",
    "    text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    words = word_tokenize(text_p)\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    arxiv_tokens[arxiv_json['id']] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_titles = list(arxiv_tokens.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading bert representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_repr_4l.pickle', 'rb') as f:\n",
    "    bert_reprs =  pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bert_reprs = []\n",
    "for article in tqdm.tqdm(list(arxiv_tokens.values())):\n",
    "    inputs = tokenizer(article,return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        hidden_states = model(**inputs)[2]\n",
    "        \n",
    "    article_embedding = torch.mean(token_vecs, dim=(0, 1))\n",
    "    bert_representation = article_embedding.detach().numpy()\n",
    "    bert_reprs.append(bert_representation)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('bert_repr_4l.pickle', 'wb') as f:\n",
    "    pickle.dump(bert_reprs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43091it [00:00, 974332.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_bert = {}\n",
    "for doc_title, bert_vect in tqdm.tqdm(zip(arxiv_titles,  bert_reprs)):\n",
    "    theta_bert[doc_title] = bert_vect\n",
    "evaluate_quality(theta_bert, dot_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43091it [00:02, 20087.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_bert_softmax = {}\n",
    "for doc_title, bert_vect in tqdm.tqdm(zip(arxiv_titles,  bert_reprs)):\n",
    "    theta_bert_softmax[doc_title] = softmax(bert_vect)\n",
    "evaluate_quality(theta_bert_softmax, cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating Bert and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_dictionary = Dictionary(list(arxiv_tokens.values()))\n",
    "arxiv_corpus = [arxiv_dictionary.doc2bow(text) for text in list(arxiv_tokens.values())]\n",
    "lda = LdaModel(arxiv_corpus, num_topics=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43091it [01:00, 708.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6953888853844699"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = {}\n",
    "for doc_title, doc_bow in tqdm.tqdm(zip(arxiv_titles, arxiv_corpus)):\n",
    "    topic_vector = np.zeros(lda.num_topics)\n",
    "    for topic_num, topic_prob in lda.get_document_topics(doc_bow):\n",
    "        topic_vector[topic_num] = topic_prob\n",
    "    theta[doc_title] = topic_vector\n",
    "evaluate_quality(theta, cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA + BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43091it [01:04, 667.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6178010471204188"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = {}\n",
    "for doc_title, doc_bow, bert_vect in tqdm.tqdm(zip(arxiv_titles, arxiv_corpus, bert_reprs)):\n",
    "    topic_vector = np.zeros(lda.num_topics)\n",
    "    for topic_num, topic_prob in lda.get_document_topics(doc_bow):\n",
    "        topic_vector[topic_num] = topic_prob\n",
    "    theta[doc_title] = np.concatenate((topic_vector, bert_vect))\n",
    "evaluate_quality(theta, cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA + softmax(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43091it [01:05, 660.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7045984987068694"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = {}\n",
    "for doc_title, doc_bow, bert_vect in tqdm.tqdm(zip(arxiv_titles, arxiv_corpus, bert_reprs)):\n",
    "    topic_vector = np.zeros(lda.num_topics)\n",
    "    for topic_num, topic_prob in lda.get_document_topics(doc_bow):\n",
    "        topic_vector[topic_num] = topic_prob\n",
    "    theta[doc_title] = np.concatenate((topic_vector, softmax(bert_vect)))\n",
    "evaluate_quality(theta, cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA on Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayazhan/anaconda3/lib/python3.8/site-packages/gensim/models/ldamodel.py:695: RuntimeWarning: overflow encountered in exp\n",
      "  expElogthetad = np.exp(Elogthetad)\n",
      "/Users/ayazhan/anaconda3/lib/python3.8/site-packages/gensim/models/ldamodel.py:693: RuntimeWarning: invalid value encountered in multiply\n",
      "  gammad = self.alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n"
     ]
    }
   ],
   "source": [
    "bert_sparse = sparse.csr_matrix(np.array(bert_reprs).T)\n",
    "corpus = Sparse2Corpus(bert_sparse)\n",
    "lda_bert = LdaModel(corpus, num_topics=300, chunksize = 50000, minimum_probability=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_bert.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sparse = sparse.csr_matrix(softmax(np.array(bert_reprs), axis=1).T)\n",
    "corpus = Sparse2Corpus(bert_sparse)\n",
    "lda_bert = LdaModel(corpus, num_topics=300, chunksize = 50000, minimum_probability=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43091it [01:19, 544.39it/s]\n"
     ]
    }
   ],
   "source": [
    "theta = {}\n",
    "for doc_title, doc_bow in tqdm.tqdm(zip(arxiv_titles, corpus)):\n",
    "    topic_vector = np.zeros(lda_bert.num_topics)\n",
    "    for topic_num, topic_prob in lda_bert.get_document_topics(doc_bow):\n",
    "        topic_vector[topic_num] = topic_prob\n",
    "    theta[doc_title] = topic_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_quality(theta, cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
