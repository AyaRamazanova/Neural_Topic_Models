{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекомендация статей с помощью тематических моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from collections import OrderedDict\n",
    "from gensim.models.ldamodel import LdaModel, CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_tokens = OrderedDict()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "with open('data/arxiv_plain.txt', 'r') as f:\n",
    "    for line in tqdm.tqdm(f):\n",
    "        cur_tokens = line.split()\n",
    "        arxiv_tokens[cur_tokens[0]] = list(filter(lambda token: token not in stop_words, cur_tokens[1:]))\n",
    "arxiv_titles = list(arxiv_tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_dictionary = Dictionary(list(arxiv_tokens.values()))\n",
    "arxiv_corpus = [arxiv_dictionary.doc2bow(text) for text in list(arxiv_tokens.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(arxiv_corpus, num_topics=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.get_document_topics(arxiv_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = {}\n",
    "for doc_title, doc_bow in tqdm.tqdm(zip(arxiv_titles, arxiv_corpus)):\n",
    "    topic_vector = np.zeros(lda.num_topics)\n",
    "    for topic_num, topic_prob in lda.get_document_topics(doc_bow):\n",
    "        topic_vector[topic_num] = topic_prob\n",
    "    theta[doc_title] = topic_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тематический вектор статьи с номером 0704.0004:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta['0704.0004']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь для того, чтобы порекомендовать читателю близкие по смыслу статьи, достаточно выбрать метрику близости и сравнить вектор текущего документа (например, последнего прочитанного) с векторами всех остальных документов в коллекции. В качестве метрики близости можно использовать косинусную меру, евклидово расстояние, расстояние Хелингера и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(first, second):\n",
    "    return metrics.pairwise.cosine_similarity(first.reshape(1, -1), second.reshape(1, -1))[0][0]\n",
    "\n",
    "def dot_sim(first, second):\n",
    "    return first.dot(second)\n",
    "\n",
    "def hel_sim(first, second): #one more sqrt and division by sqrt(2) omitted, minus added\n",
    "    return -np.sum((np.sqrt(first) - np.sqrt(second)) ** 2)\n",
    "\n",
    "def jaccard_sim(first, second):\n",
    "    intersection = set(first).intersection(set(second))\n",
    "    union = set(first).union(set(second))\n",
    "    return float(len(intersection))/float(len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_papers(query, theta, sim=cos_sim, top_k=10):\n",
    "    query_vec = theta[query]\n",
    "    ranked_list = []\n",
    "    for doc_name, doc_vec in theta.items():\n",
    "        ranked_list.append((doc_name, sim(query_vec, doc_vec)))\n",
    "    ranked_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    return ranked_list[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_papers = recommend_papers('0704.2596', theta, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_name, prob in recommended_papers:\n",
    "    print(paper_name)\n",
    "    print(' '.join([token[0] for token in Counter(arxiv_tokens[paper_name]).most_common(10)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки качества полученной рекомендательной системы воспользуемся датасетом триплетов [[Dai et al. 2015](https://arxiv.org/abs/1507.07998)]. Датасет содержит тройки статей `<запрос>|<релевантная статья>|<нерелевантная статья>`. Будем считать, что если метрика близости между запросом и релевантной статьей оказалась выше, чем между запросом и нерелевантной статьей, то такая тройка обработана \"правильно\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_quality(theta, sim):\n",
    "    all_triplets = 0\n",
    "    covered_triplets = 0\n",
    "    correct_triplets = 0\n",
    "    with open('data/arxiv_triplets.txt', 'r') as fin:\n",
    "        for line in fin:\n",
    "            ids = list(map(lambda x: x.split('/pdf/')[-1], line.split()))\n",
    "            if all([x in theta.keys() for x in ids]):\n",
    "                covered_triplets += 1\n",
    "                vectors = [theta[x] for x in ids]\n",
    "                correct_triplets += sim(vectors[0], vectors[1]) > sim(vectors[0], vectors[2])\n",
    "            all_triplets += 1\n",
    "    return 1.0 * correct_triplets / covered_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_quality(theta, cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_quality(theta, hel_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_quality(theta, dot_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем 300 тем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(arxiv_corpus, num_topics=300)\n",
    "\n",
    "theta = {}\n",
    "for doc_title, doc_bow in tqdm.tqdm(zip(arxiv_titles, arxiv_corpus)):\n",
    "    topic_vector = np.zeros(lda.num_topics)\n",
    "    for topic_num, topic_prob in lda.get_document_topics(doc_bow):\n",
    "        topic_vector[topic_num] = topic_prob\n",
    "    theta[doc_title] = topic_vector\n",
    "    \n",
    "print(evaluate_quality(theta, cos_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперимент №2: использовать BERT-based фичи совместно с тематическими фичами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from scipy import sparse\n",
    "import torch\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT: http://jalammar.github.io/illustrated-bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43091it [01:40, 428.90it/s] \n"
     ]
    }
   ],
   "source": [
    "articles =[]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "with open('data/arxiv_plain.txt', 'r') as f:\n",
    "    for line in tqdm.tqdm(f):\n",
    "        cur_tokens = line.split()\n",
    "        articles.append(' '.join(list(filter(lambda token: token not in stop_words, cur_tokens[1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles = [' '.join(tokens) for tokens in arxiv_tokens.values()]\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43091/43091 [19:30:33<00:00,  1.63s/it]   \n"
     ]
    }
   ],
   "source": [
    "bert_reprs = []\n",
    "for article in tqdm.tqdm(articles):\n",
    "    inputs = tokenizer(article,return_tensors=\"pt\", \n",
    "                padding='max_length', truncation=True, max_length = 512)\n",
    "    outputs = model(**inputs)\n",
    "    bert_representation = torch.mean(outputs.last_hidden_state, dim=1).detach().numpy() \n",
    "    bert_reprs.append(bert_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_repr.pickle', 'wb') as f:\n",
    "    pickle.dump(bert_reprs, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('bert_repr.pickle', 'rb') as f:\n",
    "    bert_reprs =  pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(bert_reprs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sparse = sparse.csr_matrix(np.array(bert_reprs)[:, 0, :].T)\n",
    "corpus = Sparse2Corpus(bert_sparse)\n",
    "lda = LdaModel(corpus, num_topics=300, chunksize = 50000, minimum_probability=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_titles = []\n",
    "with open('data/arxiv_plain.txt', 'r') as f:\n",
    "    for line in tqdm.tqdm(f):\n",
    "        cur_tokens = line.split()\n",
    "        arxiv_titles.append(cur_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = {}\n",
    "for doc_title, doc_bow in tqdm.tqdm(zip(arxiv_titles, corpus)):\n",
    "    topic_vector = np.zeros(lda.num_topics)\n",
    "    for topic_num, topic_prob in lda.get_document_topics(doc_bow):\n",
    "        topic_vector[topic_num] = topic_prob\n",
    "    theta[doc_title] = topic_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
