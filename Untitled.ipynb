{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import argparse\n",
    "import logging\n",
    "from models import WLDA\n",
    "from utils import *\n",
    "from dataset import DocDataset\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskname = 'cnews10k'\n",
    "no_below = 5\n",
    "no_above = 0.013\n",
    "num_epochs = 500\n",
    "n_topic = 20\n",
    "n_cpu = cpu_count()-2 if cpu_count()>2 else 2\n",
    "bkpt_continue = False\n",
    "use_tfidf = False\n",
    "rebuild = True\n",
    "dist = 'gmm_std'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "  0%|          | 0/10000 [00:00&lt;?, ?it/s]Tokenizing ...\n100%|██████████| 10000/10000 [00:03&lt;00:00, 2580.70it/s]\nProcessed 9884 documents.\nvoc size: 3268\n"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "docSet = DocDataset(taskname,no_below=no_below,no_above=no_above,rebuild=rebuild,use_tfidf=False)\n",
    "voc_size = docSet.vocabsize\n",
    "print('voc size:',voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WLDA(bow_dim=voc_size,n_topic=n_topic,device=device,dist=dist,taskname=taskname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch   1\tIter   10\tLoss:39.0377007\tRec Loss:35.0796051\tMMD:3.9580939\nEpoch   1\tIter   20\tLoss:48.5695175\tRec Loss:35.6720002\tMMD:12.8975173\nEpoch   2\tIter   10\tLoss:36.5756721\tRec Loss:33.9272842\tMMD:2.6483862\nEpoch   2\tIter   20\tLoss:37.9441168\tRec Loss:31.5470941\tMMD:6.3970227\nEpoch   3\tIter   10\tLoss:35.3203850\tRec Loss:33.2118683\tMMD:2.1085162\nEpoch   3\tIter   20\tLoss:43.0878092\tRec Loss:36.0338792\tMMD:7.0539316\nEpoch   4\tIter   10\tLoss:34.9536018\tRec Loss:33.0934906\tMMD:1.8601100\nEpoch   4\tIter   20\tLoss:37.9728221\tRec Loss:32.5374725\tMMD:5.4353497\nEpoch   5\tIter   10\tLoss:34.4813805\tRec Loss:32.9950600\tMMD:1.4863191\nEpoch   5\tIter   20\tLoss:35.7393768\tRec Loss:31.6003324\tMMD:4.1390428\nEpoch   6\tIter   10\tLoss:33.9281731\tRec Loss:32.7917175\tMMD:1.1364555\nEpoch   6\tIter   20\tLoss:37.9359882\tRec Loss:34.3151042\tMMD:3.6208848\nEpoch   7\tIter   10\tLoss:32.7270050\tRec Loss:31.8809834\tMMD:0.8460210\nEpoch   7\tIter   20\tLoss:34.8931165\tRec Loss:32.1471041\tMMD:2.7460118\nEpoch   8\tIter   10\tLoss:32.9984093\tRec Loss:32.3274727\tMMD:0.6709356\nEpoch   8\tIter   20\tLoss:34.3899959\tRec Loss:32.2622008\tMMD:2.1277957\nEpoch   9\tIter   10\tLoss:32.6166801\tRec Loss:32.0322914\tMMD:0.5843873\nEpoch   9\tIter   20\tLoss:34.9334998\tRec Loss:32.8164376\tMMD:2.1170625\nEpoch  10\tIter   10\tLoss:32.8048897\tRec Loss:32.2839012\tMMD:0.5209894\nEpoch  10\tIter   20\tLoss:33.3928974\tRec Loss:31.8325790\tMMD:1.5603171\nEpoch  10\tLoss:32.2540122\n[&#39;获&#39;, &#39;日本&#39;, &#39;图文&#39;, &#39;手机&#39;, &#39;世界&#39;, &#39;美&#39;, &#39;死亡&#39;, &#39;版&#39;, &#39;网络&#39;, &#39;网游&#39;, &#39;正式&#39;, &#39;曝&#39;, &#39;公司&#39;, &#39;发布&#39;, &#39;微博&#39;]\n[&#39;日本&#39;, &#39;手机&#39;, &#39;获&#39;, &#39;曝光&#39;, &#39;美&#39;, &#39;死亡&#39;, &#39;版&#39;, &#39;英国&#39;, &#39;发布&#39;, &#39;图文&#39;, &#39;�&#39;, &#39;网游&#39;, &#39;正式&#39;, &#39;活动&#39;, &#39;女子&#39;]\n[&#39;获&#39;, &#39;公布&#39;, &#39;首&#39;, &#39;美&#39;, &#39;死&#39;, &#39;余&#39;, &#39;20&#39;, &#39;欲&#39;, &#39;世界&#39;, &#39;学生&#39;, &#39;发生&#39;, &#39;上市&#39;, &#39;公司&#39;, &#39;上海&#39;, &#39;送&#39;]\n[&#39;获&#39;, &#39;日本&#39;, &#39;手机&#39;, &#39;死亡&#39;, &#39;美&#39;, &#39;图文&#39;, &#39;版&#39;, &#39;曝光&#39;, &#39;英国&#39;, &#39;正式&#39;, &#39;�&#39;, &#39;网游&#39;, &#39;世界&#39;, &#39;微博&#39;, &#39;发布&#39;]\n[&#39;曝光&#39;, &#39;手机&#39;, &#39;�&#39;, &#39;微博&#39;, &#39;日本&#39;, &#39;获&#39;, &#39;OL&#39;, &#39;女子&#39;, &#39;死亡&#39;, &#39;活动&#39;, &#39;发布&#39;, &#39;英国&#39;, &#39;曝&#39;, &#39;网游&#39;, &#39;美&#39;]\n[&#39;平起&#39;, &#39;开盘&#39;, &#39;考研&#39;, &#39;期货&#39;, &#39;别墅&#39;, &#39;沪&#39;, &#39;涨&#39;, &#39;精装&#39;, &#39;盘&#39;, &#39;现房&#39;, &#39;预计&#39;, &#39;招生&#39;, &#39;震荡&#39;, &#39;2011&#39;, &#39;享&#39;]\n[&#39;日本&#39;, &#39;获&#39;, &#39;手机&#39;, &#39;曝光&#39;, &#39;死亡&#39;, &#39;网游&#39;, &#39;版&#39;, &#39;美&#39;, &#39;微博&#39;, &#39;发布&#39;, &#39;�&#39;, &#39;女子&#39;, &#39;英国&#39;, &#39;正式&#39;, &#39;OL&#39;]\n[&#39;公布&#39;, &#39;美&#39;, &#39;首&#39;, &#39;获&#39;, &#39;市场&#39;, &#39;上市&#39;, &#39;欲&#39;, &#39;20&#39;, &#39;美元&#39;, &#39;经济&#39;, &#39;投资&#39;, &#39;上海&#39;, &#39;专家&#39;, &#39;世界&#39;, &#39;银行&#39;]\n[&#39;美&#39;, &#39;获&#39;, &#39;世界&#39;, &#39;手机&#39;, &#39;日本&#39;, &#39;20&#39;, &#39;香港&#39;, &#39;曝&#39;, &#39;正式&#39;, &#39;公司&#39;, &#39;调查&#39;, &#39;12&#39;, &#39;余&#39;, &#39;版&#39;, &#39;发布&#39;]\n[&#39;曝光&#39;, &#39;女子&#39;, &#39;日本&#39;, &#39;OL&#39;, &#39;手机&#39;, &#39;微博&#39;, &#39;获&#39;, &#39;活动&#39;, &#39;网游&#39;, &#39;�&#39;, &#39;英国&#39;, &#39;死亡&#39;, &#39;发布&#39;, &#39;版&#39;, &#39;女&#39;]\n[&#39;获&#39;, &#39;日本&#39;, &#39;世界&#39;, &#39;美&#39;, &#39;公司&#39;, &#39;死亡&#39;, &#39;网络&#39;, &#39;发生&#39;, &#39;调查&#39;, &#39;香港&#39;, &#39;未&#39;, &#39;家&#39;, &#39;图文&#39;, &#39;收购&#39;, &#39;死&#39;]\n[&#39;市场&#39;, &#39;投资&#39;, &#39;公布&#39;, &#39;美元&#39;, &#39;现&#39;, &#39;首&#39;, &#39;2011&#39;, &#39;分&#39;, &#39;第一&#39;, &#39;曼联&#39;, &#39;难&#39;, &#39;上海&#39;, &#39;基&#39;, &#39;推出&#39;, &#39;经济&#39;]\n[&#39;获&#39;, &#39;日本&#39;, &#39;美&#39;, &#39;曝光&#39;, &#39;手机&#39;, &#39;死亡&#39;, &#39;网游&#39;, &#39;图文&#39;, &#39;女子&#39;, &#39;英国&#39;, &#39;�&#39;, &#39;微博&#39;, &#39;正式&#39;, &#39;版&#39;, &#39;曝&#39;]\n[&#39;曝光&#39;, &#39;女子&#39;, &#39;OL&#39;, &#39;微博&#39;, &#39;手机&#39;, &#39;�&#39;, &#39;死亡&#39;, &#39;活动&#39;, &#39;日本&#39;, &#39;女&#39;, &#39;英国&#39;, &#39;获&#39;, &#39;网游&#39;, &#39;网友&#39;, &#39;曝&#39;]\n[&#39;日本&#39;, &#39;获&#39;, &#39;死亡&#39;, &#39;手机&#39;, &#39;图文&#39;, &#39;微博&#39;, &#39;网游&#39;, &#39;美&#39;, &#39;世界&#39;, &#39;曝光&#39;, &#39;正式&#39;, &#39;英国&#39;, &#39;网络&#39;, &#39;曝&#39;, &#39;版&#39;]\n[&#39;曝光&#39;, &#39;日本&#39;, &#39;获&#39;, &#39;死亡&#39;, &#39;手机&#39;, &#39;�&#39;, &#39;美&#39;, &#39;图文&#39;, &#39;微博&#39;, &#39;版&#39;, &#39;女子&#39;, &#39;世界&#39;, &#39;活动&#39;, &#39;发布&#39;, &#39;英国&#39;]\n[&#39;OL&#39;, &#39;女子&#39;, &#39;曝光&#39;, &#39;妻子&#39;, &#39;拍&#39;, &#39;女&#39;, &#39;网友&#39;, &#39;活动&#39;, &#39;否认&#39;, &#39;微博&#39;, &#39;女孩&#39;, &#39;丈夫&#39;, &#39;媒体&#39;, &#39;�&#39;, &#39;儿子&#39;]\n[&#39;日本&#39;, &#39;获&#39;, &#39;手机&#39;, &#39;曝光&#39;, &#39;死亡&#39;, &#39;英国&#39;, &#39;女子&#39;, &#39;微博&#39;, &#39;网游&#39;, &#39;版&#39;, &#39;OL&#39;, &#39;图文&#39;, &#39;活动&#39;, &#39;发布&#39;, &#39;女&#39;]\n[&#39;日本&#39;, &#39;获&#39;, &#39;美&#39;, &#39;手机&#39;, &#39;死亡&#39;, &#39;图文&#39;, &#39;版&#39;, &#39;世界&#39;, &#39;曝光&#39;, &#39;正式&#39;, &#39;英国&#39;, &#39;网游&#39;, &#39;发布&#39;, &#39;香港&#39;, &#39;�&#39;]\n[&#39;女子&#39;, &#39;曝光&#39;, &#39;OL&#39;, &#39;活动&#39;, &#39;手机&#39;, &#39;微博&#39;, &#39;�&#39;, &#39;女&#39;, &#39;日本&#39;, &#39;英国&#39;, &#39;死亡&#39;, &#39;网友&#39;, &#39;媒体&#39;, &#39;否认&#39;, &#39;网游&#39;]\n==============================\nCalc topic diversity:\ntd_score:0.25\nCalc topic coherence:\nTraining a word2vec model 20 epochs to evaluate topic coherence, this may take a few minutes ...\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m&lt;ipython-input-5-135888bfb108&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[1;34m\u001b[0m\n\u001b[1;32m----&gt; 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocSet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocSet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\D_Documents\\repos\\Neural_Topic_Models\\models\\WLDA.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_data, batch_size, learning_rate, test_data, num_epochs, is_evaluate, log_every, beta)\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m&#39;wlda_trainloss.png&#39;\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---&gt; 95\u001b[1;33m                     \u001b[1;33m(\u001b[0m\u001b[0mc_v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_w2v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_uci\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_npmi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmimno_tc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m                     \u001b[0mc_v_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_w2v_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_uci_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_uci\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_npmi_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_npmi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmimno_tc_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmimno_tc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mscrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m&#39;c_v&#39;\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mc_v_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m&#39;c_w2v&#39;\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mc_w2v_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m&#39;c_uci&#39;\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mc_uci_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m&#39;c_npmi&#39;\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mc_npmi_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m&#39;mimno_tc&#39;\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmimno_tc_lst\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\D_Documents\\repos\\Neural_Topic_Models\\models\\WLDA.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, test_data, calc4each)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mcv_per_topic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_w2v_per_topic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_uci_per_topic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_npmi_per_topic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             calc_topic_coherence(topic_words=topic_words, docs=test_data.docs, dictionary=test_data.dictionary,\n\u001b[1;32m--&gt; 112\u001b[1;33m                                  emb_path=None, taskname=self.taskname, sents4emb=test_data, calc4each=calc4each)\n\u001b[0m\u001b[0;32m    113\u001b[0m         print(&#39;c_v:{}, c_w2v:{}, c_uci:{}, c_npmi:{}&#39;.format(\n\u001b[0;32m    114\u001b[0m             c_v, c_w2v, c_uci, c_npmi))\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "model.train(train_data=docSet,batch_size=512,test_data=docSet,num_epochs=num_epochs,log_every=10,beta=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}